{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Myrtle_Cifar10_SGD_Nesterov_minibatches_piecewise_linear_LR_stop_osc_epoch_foriloop_weight_decay.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1A0Tuq5UEuoSjdZp6IaD9EWNhzJ-vk_-i\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import sys\n",
        "import time\n",
        "import jax\n",
        "from jax import grad, jit\n",
        "from jax.tree_util import tree_multimap\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.experimental import stax\n",
        "from jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, LogSoftmax\n",
        "import functools\n",
        "from jax.experimental import optimizers\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import random as random_1\n",
        "import numpy as np\n",
        "from jax.tree_util import tree_flatten\n",
        "\n",
        "# Utilities for working with tree-like container data structures.\n",
        "# This module provides a small set of utility functions for working with tree-like data structures,\n",
        "# such as nested tuples, lists, and dicts. We call these structures pytrees. They are trees\n",
        "# in that they are defined recursively (any non-pytree is a pytree, i.e. a leaf,\n",
        "# and any pytree of pytrees is a pytree) and can be operated on recursively\n",
        "# (object identity equivalence is not preserved by mapping operations, and the structures cannot contain\n",
        "# reference cycles).\n",
        "# The primary purpose of this module is to enable the interoperability between user defined data\n",
        "# structures and JAX transformations (e.g. jit). This is not meant to be a general purpose\n",
        "# tree-like data structure handling library.\n",
        "\n",
        "\n",
        "### TAKING A SUBSET OF CIFAR10 ###\n",
        "def make_dataset_cifar10(num_samples, classes_to_classify, C):\n",
        "\n",
        "  '''\n",
        "  :param num_samples: number of samples taken for the newYtrain and newXtrain arrays (<= size of CIFAR 10)\n",
        "  :param classes_to_classify: from the type of classes [0, 1, 2...10] of the data, focus in one in particular\n",
        "  :param C:\n",
        "  :return:\n",
        "  '''\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()  # 50 k for training, # 10 k for testing\n",
        "  x_train, x_test = x_train / 255.0, x_test / 255.0  # normalization\n",
        "\n",
        "  PosPerLabel = []  # list that stores the training example indexes ordered by their class\n",
        "  PosPerLabel.append(np.where(y_train == 0)[0])\n",
        "  for j in range(1, 10):\n",
        "      PosPerLabel.append(np.where(y_train == j)[0])\n",
        "\n",
        "  img = np.full(\n",
        "      shape=num_samples,  # 1 - D array\n",
        "      fill_value=22,\n",
        "      dtype=np.int)\n",
        "\n",
        "  weights = [1 for phi in classes_to_classify]  # weights for random selection of the classes!!\n",
        "  for t in range(num_samples):\n",
        "      img[t] = random_1.choices(classes_to_classify, weights)[0]\n",
        "\n",
        "  # Allocates space for the new training sets\n",
        "\n",
        "  newXtrain = np.full(\n",
        "  shape=(num_samples, np.shape(x_train)[1], np.shape(x_train)[2], np.shape(x_train)[3]),  # 1, 2, and 3 are the RGB channel, the X position and the Y position in the picture\n",
        "  fill_value=0,\n",
        "  dtype=np.float)    # filled with zeros. Float because it will be normalized btw (0, 1)\n",
        "\n",
        "  newYtrain = np.full(\n",
        "  shape=num_samples,\n",
        "  fill_value=0,\n",
        "  dtype=np.float)\n",
        "\n",
        "  for jj in range(num_samples):   # creates the new training subset, randomly selected\n",
        "\n",
        "      indImg = random_1.choices(PosPerLabel[img[jj]])\n",
        "      newXtrain[jj] = x_train[indImg]\n",
        "      newYtrain[jj] = y_train[indImg]\n",
        "      \n",
        "  x_train = newXtrain\n",
        "  y_train = newYtrain\n",
        "\n",
        "  # for aa in range(10):\n",
        "  #   plt.imshow(x_train[aa,:,:,:])  (M, N, 3):\n",
        "  # array-like or PIL image The image data. Supported array shapes are: an image with RGB values\n",
        "  #  (0-1 float or 0-255 int).\n",
        "  #   plt.show()\n",
        "  \n",
        "  PosPerLabel_2 = []\n",
        "  PosPerLabel_2.append(np.where(y_test == 0)[0])\n",
        "  for j in range(1,10):    \n",
        "      PosPerLabel_2.append(np.where(y_test == j)[0])\n",
        "\n",
        "  num_test_samples = 0\n",
        "  for aa in classes_to_classify:\n",
        "      num_test_samples += np.shape(PosPerLabel_2[aa])[0]\n",
        "\n",
        "  # all test samples from cifar10.load_data that have a class in classes to classify  will be taken\n",
        "\n",
        "  newXtest = np.full(\n",
        "  shape=(num_test_samples, np.shape(x_test)[1], np.shape(x_test)[2], np.shape(x_train)[3]),\n",
        "  fill_value=0,\n",
        "  dtype=np.float)\n",
        "\n",
        "  newYtest = np.full(\n",
        "  shape=num_test_samples,\n",
        "  fill_value=0,\n",
        "  dtype=np.float)\n",
        "\n",
        "  ind_im = 0\n",
        "  for jj in range(np.shape(y_test)[0]):\n",
        "\n",
        "      if y_test[jj] in classes_to_classify: \n",
        "          newXtest[ind_im] = x_test[jj]\n",
        "          newYtest[ind_im] = y_test[jj]\n",
        "          ind_im += 1\n",
        "  \n",
        "  x_test = newXtest\n",
        "  y_test = newYtest\n",
        "\n",
        "  # after this, now we have our training and test samples\n",
        "\n",
        "  y_train_vec = jnp.zeros((num_samples, C))  # like numpy zeros, allocating\n",
        "  y_train_vec = jax.ops.index_update(y_train_vec, jax.ops.index[[iii for iii in range(num_samples)], (y_train.flatten()).astype(int)], 1)\n",
        "  # y_train transformed into a 1 - D array, and stored in y_train_vect\n",
        "  y_test_vec = jnp.zeros((num_test_samples, C))\n",
        "  y_test_vec = jax.ops.index_update(y_test_vec, jax.ops.index[[iii for iii in range(num_test_samples)], (y_test.flatten()).astype(int)], 1)\n",
        "  # samw for training\n",
        "  \n",
        "  # for aa in range(10):\n",
        "  #   print(y_test[aa])\n",
        "  #   plt.imshow(x_test[aa,:,:,:])\n",
        "  #   plt.show()\n",
        "\n",
        "  '''\n",
        "  The output of device_put still acts like an NDArray, but it only copies values back to the CPU \n",
        "  when they\u2019re needed for printing, plotting, saving to disk, branching, etc.\n",
        "  The behavior of device_put is equivalent to the function jit(lambda x: x), but it\u2019s faster.\n",
        "  '''\n",
        "  return jax.device_put(x_train), jax.device_put(y_train_vec), jax.device_put(x_test), jax.device_put(y_test_vec)\n",
        "\n",
        "\n",
        "# # Network architecture described in \n",
        "# # Shankar et al., Neural Kernels Without Tangents, 2020.\n",
        "# # https://arxiv.org/abs/2003.02237\n",
        "\n",
        "### NORMAL INITIALIZATION ###\n",
        "\n",
        "def MyrtleNetwork_ini(depth, myrtle_width, C):\n",
        "  layer_factor = {5: [2, 1, 1], 7: [2, 2, 2], 10: [3, 3, 3]}\n",
        "  width = myrtle_width\n",
        "  activation_fn = jax.experimental.stax.Relu\n",
        "  layers = []\n",
        "  '''\n",
        "  functools.partial(func, /, *args, **keywords) Return a new partial object which when called \n",
        "  will behave like func called with the positional arguments args and keyword arguments keywords. \n",
        "  If more arguments are supplied to the call, they are appended to args. If additional keyword arguments are supplied,\n",
        "  they extend and override keywords\n",
        "  \n",
        "  jax.experimental.stax.Conv(out_chan, filter_shape, strides=None, padding='VALID', W_init=None,\n",
        "  b_init=<function normal.<locals>.init>) Layer construction function for a general convolution layer.\n",
        "  '''\n",
        "  conv = functools.partial(jax.experimental.stax.Conv, padding='SAME')  # , W_init = initializer, b_init = initializer\n",
        " \n",
        "  layers += [conv(width, (3, 3)), activation_fn] * layer_factor[depth][0]\n",
        "  layers += [jax.experimental.stax.AvgPool((2, 2), strides=(2, 2))]\n",
        "  layers += [conv(width, (3, 3)), activation_fn] * layer_factor[depth][1]\n",
        "  layers += [jax.experimental.stax.AvgPool((2, 2), strides=(2, 2))]\n",
        "  layers += [conv(width, (3, 3)), activation_fn] * layer_factor[depth][2]\n",
        "  layers += [jax.experimental.stax.AvgPool((2, 2), strides=(2, 2))] * 3\n",
        "\n",
        "  '''\n",
        "  def Flatten():\n",
        "    \"\"\"Layer construction function for flattening all but the leading dim.\"\"\"\n",
        "    def init_fun(rng, input_shape):\n",
        "      output_shape = input_shape[0], functools.reduce(op.mul, input_shape[1:], 1)\n",
        "      return output_shape, ()\n",
        "    def apply_fun(params, inputs, **kwargs):\n",
        "      return jnp.reshape(inputs, (inputs.shape[0], -1))\n",
        "    return init_fun, apply_fun\n",
        "    Flatten = Flatten()\n",
        "  \n",
        "  jax.experimental.stax.Dense(out_dim, W_init=<function variance_scaling.<locals>.init>, b_init=<function normal.<locals>.init>)[source]\n",
        "  Layer constructor function for a dense (fully-connected) layer.\n",
        "  '''\n",
        "  layers += [Flatten, Dense(C)]\n",
        "\n",
        "  '''\n",
        "  jax.experimental.stax.serial(*layers)[source]\n",
        "  Combinator for composing layers in serial.\n",
        "\n",
        "  Parameters\n",
        "  *layers \u2013 a sequence of layers, each an (init_fun, apply_fun) pair.\n",
        "\n",
        "  Returns\n",
        "  A new layer, meaning an (init_fun, apply_fun) pair, representing the serial composition\n",
        "  of the given sequence of layers.\n",
        "  '''\n",
        "\n",
        "  return jax.experimental.stax.serial(*layers)  # Returns: A new layer, meaning an (init_fun, apply_fun) pair, \n",
        "  # representing the serial composition of the given sequence of layers\n",
        "\n",
        "#### MINIBATCH GENERATOR ####\n",
        "def data_stream(batch_size, num_batches, X_data, Y_data):\n",
        "  seed = 0\n",
        "  while True:\n",
        "    # perm = rng.permutation(jnp.shape(X_data)[0])\n",
        "    '''\n",
        "    Unlike the stateful pseudorandom number generators (PRNGs) that users of NumPy and SciPy may be accustomed to,\n",
        "    JAX random functions all require an explicit PRNG state to be passed as a first argument. \n",
        "    The random state is described by two unsigned 32-bit integers that we call a key,\n",
        "    usually generated by the jax.random.PRNGKey() function: \n",
        "    \n",
        "    jax.random.permutation(key, x)[source]\n",
        "    Permute elements of an array along its first axis or return a permuted range.\n",
        "\n",
        "    If x is a multi-dimensional array, it is only shuffled along its first index.\n",
        "    '''\n",
        "    key = jax.random.PRNGKey(seed)\n",
        "    perm = jax.random.permutation(key, jnp.shape(X_data)[0])\n",
        "    seed += 1\n",
        "    for i in range(num_batches):\n",
        "      batch_idx = perm[i * batch_size:(i + 1) * batch_size]  # from the index before the \":\" to the index after that\n",
        "      yield X_data[batch_idx], Y_data[batch_idx]\n",
        "\n",
        "  # yield gives values at the precise moment, thus acting more like a generator\n",
        "\n",
        "#### MINIBATCH GENERATOR ####\n",
        "\n",
        "def data_stream_seed(batch_size, num_batches, X_data, Y_data, seed):\n",
        "  while True:\n",
        "    # perm = rng.permutation(jnp.shape(X_data)[0])\n",
        "    key = jax.random.PRNGKey(seed)\n",
        "    perm = jax.random.permutation(key, jnp.shape(X_data)[0])\n",
        "    seed += 1\n",
        "    for i in range(num_batches):\n",
        "      batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "      yield X_data[batch_idx], Y_data[batch_idx]\n",
        "\n",
        "\n",
        " ### Piecewise_linear takes the epoch and gives you the learning rate following a \n",
        " ### linear piecewise function going through the (points_x, points_y) ###\n",
        "\n",
        "def piecewise_linear(x, points_x, points_y):\n",
        "  for aaa in range(jnp.shape(points_x)[0]):\n",
        "    if points_x[aaa]-x <= 0:\n",
        "      ind = aaa\n",
        "  slope = (points_y[ind+1]-points_y[ind])/(points_x[ind+1]-points_x[ind])\n",
        "  y = points_y[ind] + (x-points_x[ind]) * slope\n",
        "  return y\n",
        "\n",
        "def main():\n",
        "\n",
        "  try:\n",
        "      get_ipython\n",
        "      CLUSTER = False\n",
        "  except:\n",
        "      CLUSTER = True\n",
        "  \n",
        "  if CLUSTER:  # in the cluster, arguments are parsed (e.g. via the linux console)\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"-so\", \"--stop_osc\", type=int )\n",
        "    parser.add_argument(\"-soae\", \"--stop_osc_at_epoch\", type=int )\n",
        "    parser.add_argument(\"-softm\", \"--start_osc_from_the_middle\", type=int )\n",
        "    parser.add_argument('-wml','--w_max_list', nargs='+', type=int)  # list\n",
        "    parser.add_argument('-np','--num_period',  type=int) \n",
        "    parser.add_argument(\"-rs\", \"--repeat_sim\", type=int)\n",
        "    parser.add_argument(\"-md\", \"--myrtle_depth\", type=int)\n",
        "    parser.add_argument(\"-mw\", \"--myrtle_width\", type=int)\n",
        "    parser.add_argument(\"-lft\", \"--loss_function_type\")\n",
        "    parser.add_argument(\"-tds\", \"--type_data_set\")\n",
        "    parser.add_argument(\"-ot\", \"--optimizer_type\" )\n",
        "    parser.add_argument(\"-mn\", \"--momentum_nest\", type=float)\n",
        "    parser.add_argument(\"-wd\", \"--weight_decay\", type=float)\n",
        "    parser.add_argument(\"-mbs\", \"--mini_batch_size\", type=int)\n",
        "    parser.add_argument('-eelr','--epochs_extrema_LR', nargs='+', type=int)  # list\n",
        "    parser.add_argument('-LRe','--LRs_extrema', nargs='+', type=float)  # list\n",
        "    parser.add_argument(\"-stsd\", \"--steps_to_save_data\", type=int)\n",
        "    parser.add_argument(\"-si\", \"--seed_initialization\", type=int)\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    ### PARAMETERS ###\n",
        "    # DEFINING THE OSCILLATIONS\n",
        "    stop_osc = args.stop_osc\n",
        "    stop_osc_at_epoch = args.stop_osc_at_epoch\n",
        "    start_osc_from_the_middle = args.start_osc_from_the_middle\n",
        "    w_max_list = args.w_max_list\n",
        "    num_periods_list = [args.num_period]\n",
        "    repeat_sim = args.repeat_sim\n",
        "\n",
        "    #MYRTLE Neural Net\n",
        "    myrtle_depth = args.myrtle_depth\n",
        "    myrtle_width = args.myrtle_width\n",
        "\n",
        "    #LOSS \n",
        "    loss_function_type = args.loss_function_type\n",
        "\n",
        "    ### DEFINING THE DATASET ###\n",
        "    type_data_set = args.type_data_set\n",
        "    if type_data_set == 'complete':\n",
        "      C = 10\n",
        "    elif type_data_set == 'customed':\n",
        "      num_samples = 2000 # number samples in training dataset\n",
        "      C = 2 #number of classes\n",
        "      classes_to_classify = [aa for aa in range(C)]  # classes included in training, right now they have to be consecutive!!\n",
        "\n",
        "    ### TRAINING THE NN ###\n",
        "    optimizer_type = args.optimizer_type\n",
        "    momentum_nest = args.momentum_nest\n",
        "    weight_decay = args.weight_decay\n",
        "    mini_batch_size = args.mini_batch_size\n",
        "    epochs_extrema_LR = args.epochs_extrema_LR\n",
        "    LRs_extrema = args.LRs_extrema\n",
        "    steps_to_save_data = args.steps_to_save_data\n",
        "    seed_initialization = args.seed_initialization\n",
        "\n",
        "  else:\n",
        "\n",
        "    ### PARAMETERS ###\n",
        "\n",
        "    # DEFINING THE OSCILLATIONS\n",
        "    stop_osc = 1  # int(sys.argv[6])\n",
        "    stop_osc_at_epoch = 6\n",
        "    start_osc_from_the_middle = 0  # int(sys.argv[7])\n",
        "    w_max_list = [1]\n",
        "    num_periods_list = [11]  # [11,31,51,101]\n",
        "    repeat_sim = 1\n",
        "\n",
        "    # MYRTLE Neural Net\n",
        "    myrtle_depth = 5  # 10 #int(sys.argv[9])\n",
        "    myrtle_width = 64  # int(sys.argv[10])\n",
        "\n",
        "    #LOSS \n",
        "    loss_function_type = 'cross_entropy'  # 'MSE_old' #  'old' #'old'  'MSE' #\n",
        "\n",
        "    ### DEFINING THE DATASET ###\n",
        "    type_data_set = 'complete' # 'customed'  \n",
        "    if type_data_set == 'complete':\n",
        "      C = 10\n",
        "    elif type_data_set == 'customed':\n",
        "      num_samples = 1280  # number samples in training dataset\n",
        "      C = 2  # number of classes\n",
        "      classes_to_classify = [aa for aa in range(C)]  # classes included in training, right now they have to be consecutive!!\n",
        "\n",
        "    ### TRAINING THE NN ###\n",
        "    optimizer_type = 'nesterov'  # 'sgd' #\n",
        "    momentum_nest = 0.9  # only for 'nesterov'\n",
        "    mini_batch_size = 128  # 1024\n",
        "    weight_decay = 0.0  # 0.0005*mini_batch_size\n",
        "    epochs_extrema_LR = [0, 10, 30]  # [0, 15, 30, 35] #[30,15] #[15,15,15,15] #[5000] #\n",
        "    LRs_extrema = [0.0, 0.04, 0.002]  # [0.1,0.01,0.001,0.0001] #[0.1] #\n",
        "    steps_to_save_data = 100\n",
        "    seed_initialization = 1  # int(sys.argv[13])\n",
        "\n",
        "\n",
        "  if type_data_set == 'complete':\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "    num_samples = jnp.shape(x_train)[0]\n",
        "    num_test_samples = jnp.shape(x_test)[0]\n",
        "\n",
        "    C = 10\n",
        "    classes_to_classify = [aaa for aaa in range(C)]\n",
        "    \n",
        "    y_train_vec = jnp.zeros((num_samples, C))\n",
        "    y_train_vec = jax.ops.index_update(y_train_vec, jax.ops.index[[iii for iii in range(num_samples)], y_train.flatten()], 1)\n",
        "    y_test_vec = jnp.zeros((num_test_samples, C))\n",
        "    y_test_vec = jax.ops.index_update(y_test_vec, jax.ops.index[[iii for iii in range(num_test_samples)], y_test.flatten()], 1)\n",
        "  \n",
        "    X_data, Y_data, X_test, Y_test = jax.device_put(x_train), jax.device_put(y_train_vec), jax.device_put(x_test), jax.device_put(y_test_vec)\n",
        "\n",
        "  elif type_data_set == 'customed':\n",
        "    X_data, Y_data, X_test, Y_test = make_dataset_cifar10(num_samples, classes_to_classify, C)\n",
        "\n",
        "  #### HALF PRECISION ###\n",
        "  X_data = jax.device_put(jnp.array(X_data, jnp.float16))\n",
        "  X_test = jax.device_put(jnp.array(X_test, jnp.float16))\n",
        "\n",
        "  print('np.shape(X_data)',jnp.shape(X_data))\n",
        "  print('np.shape(Y_data)',jnp.shape(Y_data))\n",
        "  print('np.shape(X_test)',jnp.shape(X_test))\n",
        "  print('np.shape(Y_test)',jnp.shape(Y_test))\n",
        "\n",
        "  if stop_osc and stop_osc_at_epoch > epochs_extrema_LR[-1]:\n",
        "    sys.exit('You would stop the oscillations when the simulation has alreay finished!')\n",
        "\n",
        "  ### LOOP FOR W_MAX ###\n",
        "\n",
        "  for num_periods in num_periods_list:\n",
        "\n",
        "    for sim_stats in range(repeat_sim):\n",
        "\n",
        "      for w_max in w_max_list:\n",
        "\n",
        "        print('epochs_extrema_LR', epochs_extrema_LR)\n",
        "        print('LRs_extrema', LRs_extrema)\n",
        "        \n",
        "        num_mini_batches = int(jnp.shape(X_data)[0]/mini_batch_size)\n",
        "        total_time = (epochs_extrema_LR[-1]) * num_mini_batches\n",
        "        if stop_osc:\n",
        "          total_time_osc = (stop_osc_at_epoch) * num_mini_batches\n",
        "        else:\n",
        "          total_time_osc = (epochs_extrema_LR[-1]) * num_mini_batches\n",
        "\n",
        "        T = int(float(total_time_osc)/float(num_periods))\n",
        "\n",
        "        ### Building the Neural Network ###\n",
        "        neural_net_tf = 'False'\n",
        "        if neural_net_tf == 'True':\n",
        "          init_fn, apply_fn, kernel_fn = MyrtleNetwork_ini(myrtle_depth, myrtle_width, C)\n",
        "        else:\n",
        "          init_fn, apply_fn = MyrtleNetwork_ini(myrtle_depth,myrtle_width,C)\n",
        "        \n",
        "        print('\\n \\n \\n \\n', 'NEW SIMULATION, MAX WEIGHT: ', w_max, '\\n \\n \\n \\n')\n",
        "        print('num_mini_batches per epoch: ', num_mini_batches)\n",
        "        print('total_time', total_time)\n",
        "        print('total_time_osc', total_time_osc)\n",
        "        print('num_periods', num_periods)\n",
        "        print('T', T)\n",
        "        \n",
        "        epoch_ind = 0\n",
        "        w_max_saved = w_max\n",
        "        file_name =  'T_{0}_w_max_{1}_sim_repet_{2}.pkl'.format(T, w_max, sim_stats)\n",
        "        ind = 0\n",
        "        L = []\n",
        "        L_weighted = []\n",
        "        A = []\n",
        "        A_test = []\n",
        "        W = []\n",
        "        NTK_EVALS = []\n",
        "        eigv_list = []\n",
        "        eigv_plot_times = []\n",
        "        times_saved = []\n",
        "\n",
        "        if start_osc_from_the_middle:\n",
        "          t = T // 2 \n",
        "        else:\n",
        "          t = 0\n",
        "\n",
        "        focus_class = 0\n",
        "\n",
        "        ### LOSS FUNCTIONS ###\n",
        "        @jit\n",
        "        def c_fn(t , i, w_max):\n",
        "\n",
        "          t = t % T\n",
        "          slope = 2 * (w_max - 1) / T \n",
        "          w_main_class = jnp.where(t < T / 2., 1+ t * slope, 2 * w_max - t * slope - 1)\n",
        "          res = jnp.ones(C) + (w_main_class-1) * jnp.eye(C)[i]\n",
        "          # numpy.eye(N, M=None, k=0, dtype=<class 'float'>, order='C', *, like=None)[source] Return a 2-D array\n",
        "          # with ones on the diagonal and zeros elsewhere.\n",
        "          # Parameters: N (int) Number of rows in the output. M (int), optional Number of columns in the output.\n",
        "          # If None, defaults to N. k (int), optional. Index of the diagonal: 0 (the default) refers to the main diagonal\n",
        "          # a positive value refers to an upper diagonal, and a negative value to a lower diagonal.\n",
        "          res = res / jnp.sum(res) * C  # normalizes to C value\n",
        "          return res\n",
        "        \n",
        "        @jit\n",
        "        def l2_squared(pytree):\n",
        "          leaves, _ = tree_flatten(pytree)\n",
        "          return sum([jnp.vdot(x, x) for x in leaves])\n",
        "\n",
        "        @jit \n",
        "        def weighted_loss(params, X, Y, t, i, w_max):\n",
        "          w = c_fn(t, i, w_max)\n",
        "          return -jnp.mean(jax.nn.log_softmax(apply_fn(params, X)) * Y * w) * C + weight_decay * l2_squared(params)\n",
        "\n",
        "        @jit\n",
        "        def loss_CE(params, X, Y):\n",
        "          return -jnp.mean(jax.nn.log_softmax(apply_fn(params, X)) * Y) * C + weight_decay * l2_squared(params)\n",
        "\n",
        "        @jit\n",
        "        def accuracy(params, X, Y):\n",
        "          return jnp.mean(jnp.argmax(apply_fn(params, X), axis=1) == jnp.argmax(Y, axis=1))\n",
        "\n",
        "\n",
        "        @jit\n",
        "        def loss_MSE(params, X, Y, weight_decay):\n",
        "          return jnp.mean((apply_fn(params, X) - Y)**2)*C + weight_decay * l2_squared(params)\n",
        "\n",
        "        @jit\n",
        "        def compute_grad(params, X_mini, Y_mini, weight_decay, t, i, w_max):\n",
        "          if loss_function_type == 'cross_entropy':\n",
        "            g = grad(weighted_loss)(params, X_mini, Y_mini, t, i, w_max)\n",
        "          elif loss_function_type == 'MSE':\n",
        "            if w_max != 1:\n",
        "              sys.exit('MSE only for weght = 1 (no oscillations')\n",
        "            g = grad(loss_MSE)(params, X_mini, Y_mini, weight_decay)\n",
        "          return g\n",
        "\n",
        "        ### INITIALIZATION ###\n",
        "        key = random.PRNGKey(seed_initialization)\n",
        "        _, params_ini = init_fn(key, X_data.shape)\n",
        "\n",
        "        # ### HALF PRESICION ###\n",
        "        # NOTE(sam): Explicitly cast to float16. \n",
        "        params_ini = jax.tree_map(lambda x: jnp.array(x, jnp.float16), params_ini)\n",
        "        \n",
        "        # #creating array with learning rates per time step (minibatch)\n",
        "        lr_per_step = jnp.array([piecewise_linear(iii/num_mini_batches, epochs_extrema_LR, LRs_extrema) for iii in range(total_time)], jnp.float16)\n",
        "        \n",
        "        #creating array with learning rates per time step (minibatch)\n",
        "        # lr_per_step = jnp.array([piecewise_linear(iii/num_mini_batches, epochs_extrema_LR, LRs_extrema) for iii in range(total_time)])\n",
        "        \n",
        "        #callable to get Learning Rate\n",
        "        @jit\n",
        "        def lr_callable(x):\n",
        "          return lr_per_step[x]\n",
        "\n",
        "        ### DEFINING NESTEROV OPTIMIZER###\n",
        "        if optimizer_type == 'nesterov':\n",
        "          (nest_init_fun, nest_update_fun, nest_get_params) = jax.experimental.optimizers.nesterov(lr_callable, momentum_nest)\n",
        "          opt_state = nest_init_fun(params_ini)\n",
        "\n",
        "\n",
        "        ### MINIMIZATION STARTS ###\n",
        "        t_old = time.time()\n",
        "\n",
        "        @jit\n",
        "        def step_nesterov_foriloop(_,all_params):\n",
        "          t, opt_state, focus_class, w_max, perm = all_params\n",
        "          i = t % num_mini_batches\n",
        "          batch_idx = jax.lax.dynamic_slice(perm, [i * mini_batch_size], [mini_batch_size]) #[i * mini_batch_size],[mini_batch_size]) # perm[i * mini_batch_size:(i + 1) * mini_batch_size]\n",
        "          X_mini, Y_mini = X_data[batch_idx], Y_data[batch_idx]\n",
        "          focus_class =  (focus_class + ((t-1) % (T))//(T-1) )  % C \n",
        "          params = nest_get_params(opt_state)\n",
        "          g = grad(weighted_loss)(params, X_mini, Y_mini, t, focus_class, w_max)\n",
        "          return t + 1, nest_update_fun(t, g, opt_state), focus_class, w_max, perm\n",
        "        \n",
        "\n",
        "        @jit\n",
        "        def compute_loss_acc(_, all_params):\n",
        "          i, t, focus_class, w_max, A_a, L_a, L_weighted_a, params_a = all_params\n",
        "          batch_idx = jax.lax.dynamic_slice(jnp.arange(jnp.shape(X_data)[0]), [i * mini_batch_size], [mini_batch_size]) #[i * mini_batch_size],[mini_batch_size]) # perm[i * mini_batch_size:(i + 1) * mini_batch_size]\n",
        "          X_mini, Y_mini = X_data[batch_idx], Y_data[batch_idx]\n",
        "          A_a += accuracy(params_a, X_mini, Y_mini) \n",
        "          if loss_function_type == 'cross_entropy':\n",
        "            L_a += loss_CE(params_a, X_mini, Y_mini) \n",
        "            L_weighted_a += weighted_loss(params_a, X_mini, Y_mini, t, focus_class, w_max) \n",
        "          return i+1, t, focus_class, w_max, A_a, L_a, L_weighted_a, params_a\n",
        "\n",
        "        @jit\n",
        "        def compute_loss_acc_test(_, all_params):\n",
        "          i, A_a, params_a = all_params\n",
        "          batch_idx = jax.lax.dynamic_slice(jnp.arange(jnp.shape(X_test)[0]), [i * mini_batch_size], [mini_batch_size]) #[i * mini_batch_size],[mini_batch_size]) # perm[i * mini_batch_size:(i + 1) * mini_batch_size]\n",
        "          X_mini, Y_mini = X_test[batch_idx], Y_test[batch_idx]\n",
        "          A_a += accuracy(params_a, X_mini, Y_mini) \n",
        "          return i+1, A_a, params_a\n",
        "\n",
        "        ##### TRAINING EPOCH BY EPOCH #####\n",
        "\n",
        "        seed_minibatch = 0\n",
        "        for epoch_ind_2 in range(epochs_extrema_LR[-1]):\n",
        "\n",
        "          if stop_osc and epoch_ind_2>=stop_osc_at_epoch:\n",
        "            print('\\n Minimization without oscillations now \\n')\n",
        "            w_max = 1\n",
        "\n",
        "          t_old = time.time()\n",
        "          seed_minibatch += 1\n",
        "          key = jax.random.PRNGKey(seed_minibatch)\n",
        "          perm = jax.random.permutation(key, jnp.shape(X_data)[0])\n",
        "\n",
        "          t, opt_state, focus_class, w_max, perm = jax.lax.fori_loop(0, num_mini_batches, step_nesterov_foriloop, (t, opt_state, focus_class, w_max, perm))\n",
        "\n",
        "          if optimizer_type == 'nesterov':\n",
        "            params = nest_get_params(opt_state)\n",
        "          t_new = time.time()\n",
        "\n",
        "          #### COMPUTING LOSSES AND ACCURACIES #### \n",
        "          A_a = 0.0\n",
        "          L_a = 0.0\n",
        "          L_weighted_a = 0.0\n",
        "          i=0\n",
        "          i, t, focus_class, w_max, A_a, L_a, L_weighted_a, params  = jax.lax.fori_loop(0, num_mini_batches, compute_loss_acc, (i, t, focus_class, w_max, A_a, L_a, L_weighted_a, params))\n",
        "\n",
        "          A_a_t = 0.0\n",
        "          i=0\n",
        "          num_minibat_test = jnp.shape(X_test)[0]//mini_batch_size\n",
        "          i, A_a_t, params = jax.lax.fori_loop(0, num_minibat_test, compute_loss_acc_test, (i, A_a_t, params))\n",
        "\n",
        "          times_saved += [t]\n",
        "          A += [A_a/num_mini_batches]\n",
        "          L += [L_a/num_mini_batches]\n",
        "          L_weighted += [L_weighted_a/num_mini_batches]\n",
        "          A_test += [A_a_t/num_minibat_test]\n",
        "          W += [c_fn(t, focus_class, w_max)]\n",
        "\n",
        "          t_new_2 = time.time()\n",
        "          print('Epoch', epoch_ind_2, 't', t, 'focus_class', focus_class, 'Acc: ', A[-1], 'Acc test: ',\n",
        "                A_test[-1], 'loss: ', L[-1], 'weighted loss: ', L_weighted[-1], 'minimization time: ',\n",
        "                (t_new-t_old), 'time acc and loss: ', t_new_2-t_new)\n",
        "\n",
        "\n",
        "        dictionary_data =  {'epochs_extrema_LR': epochs_extrema_LR,\n",
        "                            'LRs_extrema': LRs_extrema,\n",
        "                            'stop_osc': stop_osc,\n",
        "                            'stop_osc_at_epoch': stop_osc_at_epoch,\n",
        "                            'start_osc_from_the_middle': start_osc_from_the_middle,\n",
        "                            'T': T,\n",
        "                            'total_time': total_time,\n",
        "                            'total_time_osc': total_time_osc,\n",
        "                            'w_max': w_max_saved,\n",
        "                            'loss_function_type': loss_function_type,\n",
        "                            'num_samples': num_samples,\n",
        "                            'classes_to_classify': classes_to_classify,\n",
        "                            'L': L,\n",
        "                            'L_weighted': L_weighted,\n",
        "                            'W': W,\n",
        "                            'A': A,\n",
        "                            'A_test': A_test,\n",
        "                            'myrtle_depth': myrtle_depth,\n",
        "                            'myrtle_width': myrtle_width,\n",
        "                            'C': C,\n",
        "                            'times_saved': times_saved,\n",
        "                            'num_periods': num_periods,\n",
        "                            'repeat_sim': repeat_sim,\n",
        "                            'seed_initialization': seed_initialization,\n",
        "                            'type_data_set': type_data_set,\n",
        "                            'optimizer_type': optimizer_type,\n",
        "                            'momentum_nest': momentum_nest,\n",
        "                            'weight_decay': weight_decay,\n",
        "                            'mini_batch_size': mini_batch_size,\n",
        "                            'steps_to_save_data': steps_to_save_data\n",
        "                                }\n",
        "\n",
        "\n",
        "        if type_data_set == 'customed':\n",
        "          dictionary_data['num_samples'] = num_samples\n",
        "          dictionary_data['classes_to_classify'] = classes_to_classify\n",
        "\n",
        "        \n",
        "        seed_initialization += 1\n",
        "\n",
        "        # print('creating file\\n T, w_max,',T, w_max)\n",
        "        print('file: ', file_name)\n",
        "              \n",
        "        a_file = open(file_name, \"wb\")\n",
        "        pickle.dump(dictionary_data, a_file)\n",
        "        a_file.close()\n",
        "\n",
        "\n",
        "        t_final = time.time()\n",
        "\n",
        "        print('total time simulation: ', t_final - t_old)\n",
        "\n",
        "main()\n",
        "\n",
        "5e-4*128"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}