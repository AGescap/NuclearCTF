# -*- coding: utf-8 -*-
"""Myrtle_Cifar10_SGD_Nesterov_minibatches_piecewise_linear_LR_stop_osc_epoch_foriloop_weight_decay.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A0Tuq5UEuoSjdZp6IaD9EWNhzJ-vk_-i
"""

from tensorflow.keras.datasets import cifar10
import sys
import time
import jax
from jax import grad, jit
from jax.tree_util import tree_multimap
import jax.numpy as jnp
from jax import random
from jax.experimental import stax
from jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, LogSoftmax
import functools
from jax.experimental import optimizers
import pickle
import matplotlib.pyplot as plt
import argparse
import random as random_1
import numpy as np
from jax.tree_util import tree_flatten
import math
import numpy.random as nprd
# Utilities for working with tree-like container data structures.
# This module provides a small set of utility functions for working with tree-like data structures,
# such as nested tuples, lists, and dicts. We call these structures pytrees. They are trees
# in that they are defined recursively (any non-pytree is a pytree, i.e. a leaf,
# and any pytree of pytrees is a pytree) and can be operated on recursively
# (object identity equivalence is not preserved by mapping operations, and the structures cannot contain
# reference cycles).
# The primary purpose of this module is to enable the interoperability between user defined data
# structures and JAX transformations (e.g. jit). This is not meant to be a general purpose
# tree-like data structure handling library.


### TAKING A SUBSET OF CIFAR10 ###
def make_dataset_cifar10(num_samples, classes_to_classify, C):
  '''
  :param num_samples: number of samples taken for the newYtrain and newXtrain arrays (<= size of CIFAR 10)
  :param classes_to_classify: from the type of classes [0, 1, 2...10] of the data, focus in one in particular
  :param C:
  :return:
  '''

  (x_train, y_train), (x_test, y_test) = cifar10.load_data()  # 50 k for training, # 10 k for testing
  x_train, x_test = x_train / 255.0, x_test / 255.0  # normalization

  PosPerLabel = []  # list that stores the training example indexes ordered by their class
  PosPerLabel.append(np.where(y_train == 0)[0])
  for j in range(1, 10):
    PosPerLabel.append(np.where(y_train == j)[0])

  img = np.full(
    shape=num_samples,  # 1 - D array
    fill_value=22,
    dtype=np.int)

  weights = [1 for phi in classes_to_classify]  # weights for random selection of the classes!!
  for t in range(num_samples):
    img[t] = random_1.choices(classes_to_classify, weights)[0]

  # Allocates space for the new training sets

  newXtrain = np.full(
    shape=(num_samples, np.shape(x_train)[1], np.shape(x_train)[2], np.shape(x_train)[3]),
    # 1, 2, and 3 are the RGB channel, the X position and the Y position in the picture
    fill_value=0,
    dtype=np.float)  # filled with zeros. Float because it will be normalized btw (0, 1)

  newYtrain = np.full(
    shape=num_samples,
    fill_value=0,
    dtype=np.float)

  x_train_ord = np.full(
    shape=(num_samples, np.shape(x_train)[1], np.shape(x_train)[2], np.shape(x_train)[3]),
    fill_value=0,
    dtype=np.float)

  y_train_ord = np.full(
    shape=(num_samples, C),
    fill_value=0,
    dtype=np.float)

  howmany = np.zeros(C, dtype=int)
  howm_acum = np.zeros(C, dtype=int)
  cont_vec = np.zeros(C, dtype=int)

  for jj in range(num_samples):  # creates the new training subset, randomly selected

    indImg = random_1.choices(PosPerLabel[img[jj]])
    newXtrain[jj] = x_train[indImg]
    newYtrain[jj] = y_train[indImg]
    howmany[y_train[indImg]] += 1

  x_train = newXtrain
  y_train = newYtrain
  for kk in range(1, C):
    howm_acum[kk] = howm_acum[kk - 1] + howmany[kk - 1]

  for qq in range(0, num_samples):
    for ee in range(0, C):
      if y_train[qq] == ee:
        x_train_ord[howm_acum[ee] + cont_vec[ee]] = x_train[qq]
        y_train_ord[howm_acum[ee] + cont_vec[ee]][ee] = 1
        cont_vec[ee] += 1

        # for aa in range(10):
  #   plt.imshow(x_train[aa,:,:,:])  (M, N, 3):
  # array-like or PIL image The image data. Supported array shapes are: an image with RGB values
  #  (0-1 float or 0-255 int).
  #   plt.show()

  PosPerLabel_2 = []
  PosPerLabel_2.append(np.where(y_test == 0)[0])
  for j in range(1, 10):
    PosPerLabel_2.append(np.where(y_test == j)[0])

  num_test_samples = 0
  for aa in classes_to_classify:
    num_test_samples += np.shape(PosPerLabel_2[aa])[0]

  # all test samples from cifar10.load_data that have a class in classes to classify  will be taken

  newXtest = np.full(
    shape=(num_test_samples, np.shape(x_test)[1], np.shape(x_test)[2], np.shape(x_train)[3]),
    fill_value=0,
    dtype=np.float)

  newYtest = np.full(
    shape=num_test_samples,
    fill_value=0,
    dtype=np.float)

  ind_im = 0
  for jj in range(np.shape(y_test)[0]):

    if y_test[jj] in classes_to_classify:
      newXtest[ind_im] = x_test[jj]
      newYtest[ind_im] = y_test[jj]
      ind_im += 1

  x_test = newXtest
  y_test = newYtest

  # after this, now we have our training and test samples

  y_train_vec = jnp.zeros((num_samples, C))  # like numpy zeros, allocating
  y_train_vec = jax.ops.index_update(y_train_vec, jax.ops.index[
    [iii for iii in range(num_samples)], (y_train.flatten()).astype(int)], 1)
  # y_train transformed into a 1 - D array, and stored in y_train_vect
  y_test_vec = jnp.zeros((num_test_samples, C))
  y_test_vec = jax.ops.index_update(y_test_vec, jax.ops.index[
    [iii for iii in range(num_test_samples)], (y_test.flatten()).astype(int)], 1)
  # same for training

  # for aa in range(10):
  #   print(y_test[aa])
  #   plt.imshow(x_test[aa,:,:,:])
  #   plt.show()

  '''
  The output of device_put still acts like an NDArray, but it only copies values back to the CPU 
  when they’re needed for printing, plotting, saving to disk, branching, etc.
  The behavior of device_put is equivalent to the function jit(lambda x: x), but it’s faster.
  '''

  return jax.device_put(x_train), jax.device_put(y_train_vec), jax.device_put(x_test), jax.device_put(
    y_test_vec), jax.device_put(x_train_ord), jax.device_put(y_train_ord), howmany, howm_acum




# # Network architecture described in 
# # Shankar et al., Neural Kernels Without Tangents, 2020.
# # https://arxiv.org/abs/2003.02237

### NORMAL INITIALIZATION ###

def MyrtleNetwork_ini(depth, myrtle_width, C):
  layer_factor = {5: [2, 1, 1], 7: [2, 2, 2], 10: [3, 3, 3]}
  width = myrtle_width
  activation_fn = jax.experimental.stax.Relu
  layers = []
  '''
  functools.partial(func, /, *args, **keywords) Return a new partial object which when called 
  will behave like func called with the positional arguments args and keyword arguments keywords. 
  If more arguments are supplied to the call, they are appended to args. If additional keyword arguments are supplied,
  they extend and override keywords
  
  jax.experimental.stax.Conv(out_chan, filter_shape, strides=None, padding='VALID', W_init=None,
  b_init=<function normal.<locals>.init>) Layer construction function for a general convolution layer.
  '''
  conv = functools.partial(jax.experimental.stax.Conv, padding='SAME')  # , W_init = initializer, b_init = initializer
 
  layers += [conv(width, (3, 3)), activation_fn] * layer_factor[depth][0]
  layers += [jax.experimental.stax.AvgPool((2, 2), strides=(2, 2))]
  layers += [conv(width, (3, 3)), activation_fn] * layer_factor[depth][1]
  layers += [jax.experimental.stax.AvgPool((2, 2), strides=(2, 2))]
  layers += [conv(width, (3, 3)), activation_fn] * layer_factor[depth][2]
  layers += [jax.experimental.stax.AvgPool((2, 2), strides=(2, 2))] * 3

  '''
  def Flatten():
    """Layer construction function for flattening all but the leading dim."""
    def init_fun(rng, input_shape):
      output_shape = input_shape[0], functools.reduce(op.mul, input_shape[1:], 1)
      return output_shape, ()
    def apply_fun(params, inputs, **kwargs):
      return jnp.reshape(inputs, (inputs.shape[0], -1))
    return init_fun, apply_fun
    Flatten = Flatten()
  
  jax.experimental.stax.Dense(out_dim, W_init=<function variance_scaling.<locals>.init>, b_init=<function normal.<locals>.init>)[source]
  Layer constructor function for a dense (fully-connected) layer.
  '''
  layers += [Flatten, Dense(C)]

  '''
  jax.experimental.stax.serial(*layers)[source]
  Combinator for composing layers in serial.

  Parameters
  *layers – a sequence of layers, each an (init_fun, apply_fun) pair.

  Returns
  A new layer, meaning an (init_fun, apply_fun) pair, representing the serial composition
  of the given sequence of layers.
  '''

  return jax.experimental.stax.serial(*layers)  # Returns: A new layer, meaning an (init_fun, apply_fun) pair, 
  # representing the serial composition of the given sequence of layers

#### MINIBATCH GENERATOR ####
def data_stream(batch_size, num_batches, X_data, Y_data):
  seed = 0
  while True:
    # perm = rng.permutation(jnp.shape(X_data)[0])
    '''
    Unlike the stateful pseudorandom number generators (PRNGs) that users of NumPy and SciPy may be accustomed to,
    JAX random functions all require an explicit PRNG state to be passed as a first argument. 
    The random state is described by two unsigned 32-bit integers that we call a key,
    usually generated by the jax.random.PRNGKey() function: 
    
    jax.random.permutation(key, x)[source]
    Permute elements of an array along its first axis or return a permuted range.

    If x is a multi-dimensional array, it is only shuffled along its first index.
    '''
    key = jax.random.PRNGKey(seed)
    perm = jax.random.permutation(key, jnp.shape(X_data)[0])
    seed += 1
    for i in range(num_batches):
      batch_idx = perm[i * batch_size:(i + 1) * batch_size]  # from the index before the ":" to the index after that
      yield X_data[batch_idx], Y_data[batch_idx]

  # yield gives values at the precise moment, thus acting more like a generator

#### MINIBATCH GENERATOR ####

def data_stream_seed(batch_size, num_batches, X_data, Y_data, seed):
  while True:
    # perm = rng.permutation(jnp.shape(X_data)[0])
    key = jax.random.PRNGKey(seed)
    perm = jax.random.permutation(key, jnp.shape(X_data)[0])
    seed += 1
    for i in range(num_batches):
      batch_idx = perm[i * batch_size:(i + 1) * batch_size]
      yield X_data[batch_idx], Y_data[batch_idx]


 ### Piecewise_linear takes the epoch and gives you the learning rate following a 
 ### linear piecewise function going through the (points_x, points_y) ###

def piecewise_linear(x, points_x, points_y):
  for aaa in range(jnp.shape(points_x)[0]):
    if points_x[aaa]-x <= 0:
      ind = aaa
  slope = (points_y[ind+1]-points_y[ind])/(points_x[ind+1]-points_x[ind])
  y = points_y[ind] + (x-points_x[ind]) * slope
  return y


def main():
    # try:
    #     get_ipython
    #     CLUSTER = False
    # except:
    #     CLUSTER = True

    CLUSTER = False

    if CLUSTER:  # in the cluster, arguments are parsed (e.g. via the linux console)

        parser = argparse.ArgumentParser()

        parser.add_argument("-so", "--stop_osc", type=int)
        parser.add_argument("-soae", "--stop_osc_at_epoch", type=int)
        parser.add_argument("-softm", "--start_osc_from_the_middle", type=int)
        parser.add_argument('-np', '--num_period', type=int)
        parser.add_argument("-rs", "--repeat_sim", type=int)
        parser.add_argument("-md", "--myrtle_depth", type=int)
        parser.add_argument("-mw", "--myrtle_width", type=int)
        parser.add_argument("-lft", "--loss_function_type")
        parser.add_argument("-tds", "--type_data_set")
        parser.add_argument("-ot", "--optimizer_type")
        parser.add_argument("-mn", "--momentum_nest", type=float)
        parser.add_argument("-wd", "--weight_decay", type=float)
        parser.add_argument("-mbs", "--mini_batch_size", type=int)
        parser.add_argument('-eelr', '--epochs_extrema_LR', nargs='+', type=int)  # list
        parser.add_argument('-LRe', '--LRs_extrema', nargs='+', type=float)  # list
        parser.add_argument("-stsd", "--steps_to_save_data", type=int)
        parser.add_argument("-si", "--seed_initialization", type=int)
        parser.add_argument("-amp", "--probwaves_amplitude", type=float)  # list
        parser.add_argument("-offset", "--probwaves_offset", type=float)  # list
        parser.add_argument("-freq", "--probwaves_frequency", type=float) # list

        args = parser.parse_args()

        ### PARAMETERS ###
        # DEFINING THE OSCILLATIONS
        stop_osc = args.stop_osc
        stop_osc_at_epoch = args.stop_osc_at_epoch
        start_osc_from_the_middle = args.start_osc_from_the_middle
        num_periods_list = [args.num_period]
        repeat_sim = args.repeat_sim

        # MYRTLE Neural Net
        myrtle_depth = args.myrtle_depth
        myrtle_width = args.myrtle_width

        # LOSS
        loss_function_type = args.loss_function_type

        ### DEFINING THE DATASET ###
        type_data_set = args.type_data_set
        if type_data_set == 'complete':
            C = 10
        elif type_data_set == 'customed':
            num_samples = 500  # number samples in training dataset
            C = 4  # number of classes
            classes_to_classify = [aa for aa in
                                   range(C)]  # classes included in training, right now they have to be consecutive!!

        ### TRAINING THE NN ###
        optimizer_type = args.optimizer_type
        momentum_nest = args.momentum_nest
        weight_decay = args.weight_decay
        mini_batch_size = args.mini_batch_size
        epochs_extrema_LR = args.epochs_extrema_LR
        LRs_extrema = args.LRs_extrema
        steps_to_save_data = args.steps_to_save_data
        seed_initialization = args.seed_initialization

        ### DEFINING PROBABILITY FUNCTIONS
        probwaves_amplitude = args.probwaves_amplitude
        probwaves_offset = args.probwaves_offset
        probwaves_frequency = args.probwaves_frequency

    else:

        ### PARAMETERS ###

        # DEFINING THE OSCILLATIONS
        stop_osc = 1  # int(sys.argv[6])
        stop_osc_at_epoch = 6
        start_osc_from_the_middle = 0  # int(sys.argv[7])
        num_periods_list = [11]  # [11,31,51,101]
        repeat_sim = 1

        # MYRTLE Neural Net
        myrtle_depth = 5  # 10 #int(sys.argv[9])
        myrtle_width = 64  # int(sys.argv[10])

        # LOSS
        loss_function_type = 'cross_entropy'  # 'MSE_old' #  'old' #'old'  'MSE' #

        ### DEFINING THE DATASET ###
        type_data_set = 'customed'  # 'customed'

        if type_data_set == 'complete':
            C = 10

        elif type_data_set == 'customed':
            num_samples = 100  # number samples in training dataset
            C = 4  # number of classes
            classes_to_classify = [aa for aa in
                                   range(C)]  # classes included in training, right now they have to be consecutive!!

        ### TRAINING THE NN ###
        optimizer_type = 'nesterov'  # 'sgd' #
        momentum_nest = 0.9  # only for 'nesterov'
        mini_batch_size = 5  # 1024
        weight_decay = 0.0  # 0.0005*mini_batch_size
        epochs_extrema_LR = [0, 10, 30]  # [0, 15, 30, 35] #[30,15] #[15,15,15,15] #[5000] #
        LRs_extrema = [0.0, 0.04, 0.002]  # [0.1,0.01,0.001,0.0001] #[0.1] #
        steps_to_save_data = 100
        seed_initialization = 1  # int(sys.argv[13])

        ### DEFINING PROBABILITY FUNCTIONS
        probwaves_amplitude = [2.0, 3.0]
        probwaves_frequency = [0.25, 0.75, 1.0]
        probwaves_offset = [1.99, 2.99]

    if type_data_set == 'complete':

        (x_train, y_train), (x_test, y_test) = cifar10.load_data()
        x_train, x_test = x_train / 255.0, x_test / 255.0

        num_samples = jnp.shape(x_train)[0]
        num_test_samples = jnp.shape(x_test)[0]

        C = 10
        classes_to_classify = [aaa for aaa in range(C)]

        y_train_vec = jnp.zeros((num_samples, C))
        y_train_vec = jax.ops.index_update(y_train_vec,
                                           jax.ops.index[[iii for iii in range(num_samples)], y_train.flatten()], 1)
        y_test_vec = jnp.zeros((num_test_samples, C))
        y_test_vec = jax.ops.index_update(y_test_vec,
                                          jax.ops.index[[iii for iii in range(num_test_samples)], y_test.flatten()], 1)

        X_data, Y_data, X_test, Y_test = jax.device_put(x_train), jax.device_put(y_train_vec), jax.device_put(
            x_test), jax.device_put(y_test_vec)

        X_data_ord = np.full(
            shape=(num_samples, np.shape(x_train)[1], np.shape(x_train)[2], np.shape(x_train)[3]),
            fill_value=0,
            dtype=np.float16)

        Y_data_ord = np.full(
            shape=(num_samples, C),
            fill_value=0,
            dtype=np.float16)

        size_databatch = int(X_data.shape[0] / 10.0)
        guide1 = np.zeros(C, dtype=int)
        guide2 = np.zeros(C, dtype=int)
        cont_vec = np.zeros(C, dtype=int)

        for jj in range(0, C):
            guide1[jj] = size_databatch
            guide2[jj] = jj * size_databatch
            # TODO alright, but the 'complete' option could be further simplified in the loops below

        for kk in range(0, num_samples):
            for ee in range(0, C):
                if Y_data[kk][ee] == 1:
                    X_data_ord[guide2[ee] + cont_vec[ee]] = X_data[kk]
                    Y_data_ord[guide2[ee] + cont_vec[ee]] = Y_data[kk]
                    cont_vec[ee] += 1

        X_data_ord, Y_data_ord = jax.device_put(X_data_ord), jax.device_put(Y_data_ord)

    elif type_data_set == 'customed':
        X_data, Y_data, X_test, Y_test, X_data_ord, Y_data_ord, guide1, guide2 = make_dataset_cifar10(num_samples,
                                                                                                      classes_to_classify,
                                                                                                      C)

    X_mkbatch = jnp.full(
        shape=(num_samples, np.shape(X_data)[1], np.shape(X_data)[2], np.shape(X_data)[3]),
        fill_value=0,
        dtype=np.float16)

    Y_mkbatch = jnp.full(
        shape=(num_samples, C),
        fill_value=0,
        dtype=np.float16)

    #### HALF PRECISION ###
    X_data = jax.device_put(jnp.array(X_data, jnp.float16))
    X_test = jax.device_put(jnp.array(X_test, jnp.float16))
    X_data_ord = jax.device_put(jnp.array(X_data_ord, jnp.float16))

    # the previous operations preserve the shape
    print('np.shape(X_data)', jnp.shape(X_data))
    print('np.shape(Y_data)', jnp.shape(Y_data))
    print('np.shape(X_test)', jnp.shape(X_test))
    print('np.shape(Y_test)', jnp.shape(Y_test))
    print('np.shape(X_data_ord)', jnp.shape(X_data_ord))
    print('np.shape(Y_data_ord)', jnp.shape(Y_data_ord))

    if stop_osc and stop_osc_at_epoch > epochs_extrema_LR[
        -1]:  # now we assume that oscillations refer to the probability of choosing an option
        sys.exit('You would stop the oscillations when the simulation has already finished!')

    ### LOOP FOR W_MAX ###

    X_osc_data = np.full(
        shape=(num_samples, np.shape(X_data)[1], np.shape(X_data)[2], np.shape(X_data)[3]),
        fill_value=0,
        dtype=np.float16)

    Y_osc_data = np.full(
        shape=(num_samples, C),
        fill_value=0,
        dtype=np.float16
    )
    seed_minibatch2 = 50
    allnumsord = jnp.arange(0, num_samples)
    for num_periods in num_periods_list:  # we can use T (or better, 1/T = f as t for our

        for sim_stats in range(repeat_sim):

            for nn in range(0, len(probwaves_amplitude)):

                probamp = probwaves_amplitude[nn]
                proboffset = probwaves_offset[nn]

                for xx in range(0, len(probwaves_frequency)):

                    print('epochs_extrema_LR', epochs_extrema_LR)
                    print('LRs_extrema', LRs_extrema)
                    probfreq = probwaves_frequency[xx]
                    num_mini_batches = int(jnp.shape(X_data)[
                                               0] / mini_batch_size)  # here num_mini_batches is not an "a priori" quantity, but it depends on the path.
                    # We could define an epoch as the creation of a set of minibatches when creating a new one is not guaranteed anymore (i.e. for a given class there are not enough
                    # unused examples to cover an entire minibatch. This could be a rigid condition (we check before doing it if minibatch_size > min. remaining of a class) or interrupt

                    total_time = (epochs_extrema_LR[
                        -1]) * num_mini_batches  # num_mini_batches >= number of minibatches that
                    # will result from our iterations

                    if stop_osc:
                        total_time_osc = (stop_osc_at_epoch) * num_mini_batches
                    else:
                        total_time_osc = (epochs_extrema_LR[-1]) * num_mini_batches

                    # T = int(float(total_time_osc)/float(num_periods))

                    T = int(float(total_time_osc) / float(num_periods))

                    ### Building the Neural Network ###

                    neural_net_tf = 'False'
                    if neural_net_tf == 'True':
                        init_fn, apply_fn, kernel_fn = MyrtleNetwork_ini(myrtle_depth, myrtle_width, C)
                    else:
                        init_fn, apply_fn = MyrtleNetwork_ini(myrtle_depth, myrtle_width, C)

                    print('\n \n \n \n', 'NEW SIMULATION, FREQUENCY OF THE PROB. FUNCTION: ', probwaves_frequency,
                          '\n \n \n \n')
                    print('max number of minibatches per epoch: ', num_mini_batches)
                    print('total_time', total_time)
                    print('total_time_osc', total_time_osc)
                    print('num_periods', num_periods)
                    print('T', T)

                    epoch_ind = 0
                    file_name = 'ampl_{0}_freq_{1}_sim_repet_{2}.pkl'.format(probwaves_amplitude, probwaves_frequency,
                                                                             sim_stats)  # TODO see this
                    ind = 0
                    L = []
                    L_weighted = []
                    L_simple = []
                    A = []
                    A_test = []
                    NTK_EVALS = []
                    eigv_list = []
                    eigv_plot_times = []
                    times_saved = []

                    if start_osc_from_the_middle:  # we do not know where the middle is
                        t = T // 2
                    else:
                        t = 0

                    ### LOSS FUNCTIONS ###

                    @jit
                    def l2_squared(pytree):
                        leaves, _ = tree_flatten(pytree)
                        return sum([jnp.vdot(x, x) for x in leaves])

                    @jit
                    def loss_CE(params, X, Y):
                        return -jnp.mean(jax.nn.log_softmax(apply_fn(params, X)) * Y) * C + weight_decay * l2_squared(
                            params)

                    @jit
                    def accuracy(params, X, Y):
                        return jnp.mean(jnp.argmax(apply_fn(params, X), axis=1) == jnp.argmax(Y, axis=1))

                    @jit
                    def loss_MSE(params, X, Y, weight_decay):
                        return jnp.mean((apply_fn(params, X) - Y) ** 2) * C + weight_decay * l2_squared(params)

                    ### INITIALIZATION ###
                    key = random.PRNGKey(seed_initialization)
                    _, params_ini = init_fn(key, X_data.shape)

                    # ### HALF PRECISION ###
                    # NOTE(sam): Explicitly cast to float16.
                    params_ini = jax.tree_map(lambda x: jnp.array(x, jnp.float16), params_ini)

                    # creating array with learning rates per time step (minibatch)
                    lr_per_step = jnp.array(
                        [piecewise_linear(iii / num_mini_batches, epochs_extrema_LR, LRs_extrema) for iii in
                         range(total_time)], jnp.float16)

                    # creating array with learning rates per time step (minibatch)
                    # lr_per_step = jnp.array([piecewise_linear(iii/num_mini_batches, epochs_extrema_LR, LRs_extrema) for iii in range(total_time)])

                    # callable to get Learning Rate

                    def funcprobcos(t, a=1.0, delay=0.0, freq=1.0, offset=1.0):
                        return offset + a * math.cos(delay + 2 * math.pi * freq * t)

                    def probset(t, C, a=1.0, freq=1.0, offset=1.0, initphase=0.0):
                        res = np.zeros(C, dtype=float)
                        for mm in range(0, C):
                            res[mm] = abs(funcprobcos(t, a=a, delay=2 * math.pi * float(mm / C) + initphase, freq=freq,
                                                      offset=offset))
                        b = sum(res)
                        res = res / b
                        return res

                    @jit
                    def lr_callable(x):
                        return lr_per_step[x]

                    ### DEFINING NESTEROV OPTIMIZER###
                    if optimizer_type == 'nesterov':
                        (nest_init_fun, nest_update_fun, nest_get_params) = jax.experimental.optimizers.nesterov(
                            lr_callable, momentum_nest)
                        opt_state = nest_init_fun(params_ini)
                        '''
                        jax.experimental.optimizers.nesterov(step_size, mass)[source] Construct optimizer triple
                        for SGD with Nesterov momentum
                        Parameters:
                        step_size (Callable[[int], float]) – positive scalar, or a callable representing a step size schedule that maps the iteration index to positive scalar.
                        mass (float) – positive scalar representing the momentum coefficient.
                        Returns:
                        An (init_fun, update_fun, get_params) triple.
                        '''

                    ### MINIMIZATION STARTS ###
                    t_old = time.time()

                    @jit
                    def simple_step_nesterov_foriloop(_, all_params):
                        t, opt_state, perm = all_params
                        i = t % num_mini_batches
                        batch_idx = jax.lax.dynamic_slice(perm, [i * mini_batch_size],
                                                          [mini_batch_size])  # [i * mini_batch_size],
                        # [mini_batch_size]) # perm[i * mini_batch_size:(i + 1) * mini_batch_size]
                        X_mini, Y_mini = X_data[batch_idx], Y_data[batch_idx]
                        params = nest_get_params(opt_state)
                        g = grad(loss_CE)(params, X_mini, Y_mini)
                        return t + 1, nest_update_fun(t, g, opt_state), perm

                    @jit
                    def osc_step_nesterov_foriloop(_, all_params):
                        t, opt_state = all_params
                        i = t % num_mini_batches
                        batch_idx = jax.lax.dynamic_slice(allnumsord, [i * mini_batch_size], [mini_batch_size])
                        X_mini, Y_mini = X_mkbatch[batch_idx], Y_mkbatch[batch_idx]
                        params = nest_get_params(opt_state)
                        g = grad(loss_CE)(params, X_mini, Y_mini)
                        return t + 1, nest_update_fun(t, g, opt_state)

                    @jit
                    def simple_compute_loss_acc(_, all_params):
                        i, t, A_a, L_a, L_simple_a, params_a = all_params
                        batch_idx = jax.lax.dynamic_slice(jnp.arange(jnp.shape(X_data)[0]), [i * mini_batch_size], [
                            mini_batch_size])  # [i * mini_batch_size],[mini_batch_size]) # perm[i * mini_batch_size:(i + 1) * mini_batch_size]
                        X_mini, Y_mini = X_data[batch_idx], Y_data[batch_idx]
                        A_a += accuracy(params_a, X_mini, Y_mini)
                        if loss_function_type == 'cross_entropy':
                            L_a += loss_CE(params_a, X_mini, Y_mini)
                            L_simple_a += loss_CE(params_a, X_mini, Y_mini)
                        return i + 1, t, A_a, L_a, L_simple_a, params_a  # with i, update the batches to

                    @jit
                    def compute_loss_acc_test(_, all_params):
                        i, A_a, params_a = all_params
                        batch_idx = jax.lax.dynamic_slice(jnp.arange(jnp.shape(X_test)[0]), [i * mini_batch_size], [
                            mini_batch_size])  # [i * mini_batch_size],[mini_batch_size]) # perm[i * mini_batch_size:(i + 1) * mini_batch_size]
                        X_mini, Y_mini = X_test[batch_idx], Y_test[batch_idx]
                        A_a += accuracy(params_a, X_mini, Y_mini)
                        return i + 1, A_a, params_a

                    ##### TRAINING EPOCH BY EPOCH #####

                    seed_minibatch = 0
                    for epoch_ind_2 in range(epochs_extrema_LR[-1]):

                        if stop_osc and epoch_ind_2 >= stop_osc_at_epoch:
                            print('\n Minimization without oscillations now \n')
                            t_old = time.time()
                            seed_minibatch += 1
                            key = jax.random.PRNGKey(seed_minibatch)
                            perm = jax.random.permutation(key, jnp.shape(X_data)[
                                0])  # so that the order of access to the minibatches
                            # is random
                            t, opt_state, perm = jax.lax.fori_loop(0, num_mini_batches, simple_step_nesterov_foriloop,
                                                                   (t, opt_state, perm))

                            if optimizer_type == 'nesterov':
                                params = nest_get_params(opt_state)
                            t_new = time.time()

                            #### COMPUTING LOSSES AND ACCURACIES ####
                            A_a = 0.0
                            L_a = 0.0
                            L_simple_a = 0.0
                            i = 0
                            i, t, A_a, L_a, L_simple_a, params = jax.lax.fori_loop(0, num_mini_batches,
                                                                                   simple_compute_loss_acc,
                                                                                   (i, t, A_a,
                                                                                    L_a, L_simple_a, params))

                            A_a_t = 0.0
                            i = 0
                            num_minibat_test = jnp.shape(X_test)[0] // mini_batch_size
                            i, A_a_t, params = jax.lax.fori_loop(0, num_minibat_test, compute_loss_acc_test,
                                                                 (i, A_a_t, params))

                            times_saved += [t]
                            A += [A_a / num_mini_batches]
                            L += [L_a / num_mini_batches]
                            L_simple += [L_simple_a / num_mini_batches]
                            A_test += [A_a_t / num_minibat_test]

                            t_new_2 = time.time()
                            print('Epoch', epoch_ind_2, 't', t, 'Acc: ', A[-1], 'Acc test: ',
                                  A_test[-1], 'loss: ', L[-1], 'loss: ', L_simple[-1], 'minimization time: ',
                                  (t_new - t_old), 'time acc and loss: ', t_new_2 - t_new)


                        else:

                            t_old = time.time()
                            seed_minibatch2 += 1
                            key = jax.random.PRNGKey(seed_minibatch2)
                            n_shift = float(
                                jax.random.choice(key, C))  # an epoch should have the same number of minibatches
                            t0 = int(t)  # so that t won't change and it will increment in the foriloop
                            for ccc in range(0, num_mini_batches):
                                probs = probset(t0 + ccc, C, probamp, probfreq, proboffset, n_shift * 2 * math.pi / C)
                                seed_minibatch2 += 1
                                nprd.seed(seed_minibatch2)
                                minib_index = nprd.choice(C, size=mini_batch_size, replace=True, p=probs)
                                for eee in range(0, mini_batch_size):
                                    clasif = minib_index[eee]
                                    seed_minibatch2 += 1
                                    key = jax.random.PRNGKey(seed_minibatch2)
                                    ex_choice = jax.random.choice(key, guide1[clasif])
                                    X_osc_data[ccc * mini_batch_size + eee] = X_data_ord[guide2[clasif] + ex_choice]
                                    Y_osc_data[ccc * mini_batch_size + eee] = Y_data_ord[guide2[clasif] + ex_choice]

                            X_mkbatch = jax.device_put(X_osc_data)
                            Y_mkbatch = jax.device_put(Y_osc_data)

                            t, opt_state = jax.lax.fori_loop(0, num_mini_batches, osc_step_nesterov_foriloop,
                                                             (t, opt_state))

                            if optimizer_type == 'nesterov':
                                params = nest_get_params(opt_state)
                            t_new = time.time()

                            #### COMPUTING LOSSES AND ACCURACIES ####
                            A_a = 0.0
                            L_a = 0.0
                            L_simple_a = 0.0
                            i = 0
                            i, t, A_a, L_a, L_simple_a, params = jax.lax.fori_loop(0, num_mini_batches,
                                                                                   simple_compute_loss_acc,
                                                                                   (i, t, A_a, L_a, L_simple_a, params))
                            A_a_t = 0.0
                            i = 0
                            num_minibat_test = jnp.shape(X_test)[0] // mini_batch_size
                            i, A_a_t, params = jax.lax.fori_loop(0, num_minibat_test, compute_loss_acc_test,
                                                                 (i, A_a_t, params))

                            times_saved += [t]
                            A += [A_a / num_mini_batches]
                            L += [L_a / num_mini_batches]
                            L_simple += [L_simple_a / num_mini_batches]
                            A_test += [A_a_t / num_minibat_test]

                            t_new_2 = time.time()
                            print("I'm doing things")
                            print('Epoch', epoch_ind_2, 't', t, 'Acc: ', A[-1], 'Acc test: ',
                                  A_test[-1], 'loss: ', L[-1], 'loss: ', L_simple[-1], 'minimization time: ',
                                  (t_new - t_old), 'time acc and loss: ', t_new_2 - t_new)

                    dictionary_data = {'epochs_extrema_LR': epochs_extrema_LR,
                                       'LRs_extrema': LRs_extrema,
                                       'stop_osc': stop_osc,
                                       'stop_osc_at_epoch': stop_osc_at_epoch,
                                       'start_osc_from_the_middle': start_osc_from_the_middle,
                                       'T': T,
                                       'total_time': total_time,
                                       'total_time_osc': total_time_osc,
                                       'loss_function_type': loss_function_type,
                                       'num_samples': num_samples,
                                       'classes_to_classify': classes_to_classify,
                                       'L': L,
                                       'L_weighted': L_weighted,
                                       'A': A,
                                       'A_test': A_test,
                                       'myrtle_depth': myrtle_depth,
                                       'myrtle_width': myrtle_width,
                                       'C': C,
                                       'times_saved': times_saved,
                                       'num_periods': num_periods,
                                       'repeat_sim': repeat_sim,
                                       'seed_initialization': seed_initialization,
                                       'type_data_set': type_data_set,
                                       'optimizer_type': optimizer_type,
                                       'momentum_nest': momentum_nest,
                                       'weight_decay': weight_decay,
                                       'mini_batch_size': mini_batch_size,
                                       'steps_to_save_data': steps_to_save_data,
                                       'probwave_amplitude:': probwaves_amplitude[nn],
                                       'probwave_frequency:': probwaves_frequency[xx],
                                       'probwave_offset:': probwaves_offset[nn],
                                       }

                    if type_data_set == 'customed':
                        dictionary_data['num_samples'] = num_samples
                        dictionary_data['classes_to_classify'] = classes_to_classify

                    seed_initialization += 1

                    # print('creating file\n T, w_max,',T, w_max)
                    print('file: ', file_name)

                    a_file = open(file_name, "wb")
                    pickle.dump(dictionary_data, a_file)
                    a_file.close()

                    t_final = time.time()

                    print('total time simulation: ', t_final - t_old)


main()

